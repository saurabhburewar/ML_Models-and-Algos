{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B18CSE050_assignment6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W-yQzA2nUGP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY9eG-uA3KYx"
      },
      "source": [
        "class Custom_NN(nn.Module):\n",
        "  def __init__(self, in_features, out_features, bias=True, p=0.5):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(in_features, out_features, bias)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.drop = nn.Dropout(p)\n",
        "  def forward(self, X):\n",
        "    X = self.linear(X)\n",
        "    X = self.relu(X)\n",
        "    X = self.drop(X)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDsiPZL4mMCd"
      },
      "source": [
        "NN_class = nn.Sequential(Custom_NN(5, 32), nn.Linear(32, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNjSTO6Wn8g-"
      },
      "source": [
        "train_set = pd.read_csv('https://raw.githubusercontent.com/saurabhburewar/ML_Datasets/main/titanic_dataset/train.csv')\n",
        "test_set = pd.read_csv('https://raw.githubusercontent.com/saurabhburewar/ML_Datasets/main/titanic_dataset/test.csv')\n",
        "\n",
        "train_set.drop(['Cabin'], axis=1, inplace=True)\n",
        "train_set.dropna(how='any', inplace=True)\n",
        "train_set['sex_factor'] = pd.factorize(train_set.Sex)[0]\n",
        "train_set['em_factor'] = pd.factorize(train_set.Embarked)[0]\n",
        "\n",
        "X_train = train_set[['Pclass', 'Age', 'Fare', 'sex_factor', 'em_factor']]\n",
        "Y_train = train_set['Survived']\n",
        "\n",
        "X_train = X_train.values\n",
        "Y_train = Y_train.values\n",
        "\n",
        "test_set.drop(['Cabin'], axis=1, inplace=True)\n",
        "test_set.dropna(how='any', inplace=True)\n",
        "test_set['sex_factor'] = pd.factorize(test_set.Sex)[0]\n",
        "test_set['em_factor'] = pd.factorize(test_set.Embarked)[0]\n",
        "\n",
        "X_test = test_set[['Pclass', 'Age', 'Fare', 'sex_factor', 'em_factor']]\n",
        "\n",
        "X_test = X_test.values\n",
        "\n",
        "batch_size = 32\n",
        "iterations = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_d8f3uz-KX5"
      },
      "source": [
        "# dataset = pd.read_csv('https://raw.githubusercontent.com/saurabhburewar/ML_Datasets/main/Iris_dataset.csv')\n",
        "\n",
        "# X = dataset.drop(['species'], axis=1)\n",
        "# Y = dataset['species']\n",
        "\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n",
        "\n",
        "# X_train = X_train.values\n",
        "# Y_train = Y_train.values\n",
        "\n",
        "# X_test = X_test.values\n",
        "\n",
        "# batch_size = 32\n",
        "# iterations = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MynJD1eDee3Q"
      },
      "source": [
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True).split(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4_tp7Oedfa"
      },
      "source": [
        "train_preds = np.zeros((len(X_train)))\n",
        "test_preds = np.zeros((len(X_test)))\n",
        "\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).cuda()\n",
        "test = torch.utils.data.TensorDataset(X_test_tensor)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gyt8HtokLL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b50d388-7753-4126-ea65-bae3ad1103e3"
      },
      "source": [
        "for i_fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "  X_train_fold = torch.tensor(X_train[train_idx], dtype=torch.float32).cuda()\n",
        "  Y_train_fold = torch.tensor(Y_train[train_idx], dtype=torch.float32).unsqueeze(1).cuda()\n",
        "  X_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.float32).cuda()\n",
        "  Y_val_fold = torch.tensor(Y_train[valid_idx], dtype=torch.float32).unsqueeze(1).cuda()\n",
        "\n",
        "  train = torch.utils.data.TensorDataset(X_train_fold, Y_train_fold)\n",
        "  valid = torch.utils.data.TensorDataset(X_val_fold, Y_val_fold)\n",
        "\n",
        "  trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "  validloader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  model = NN_class\n",
        "  NN_class.cuda()\n",
        "  loss = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
        "  op = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  print(\"Fold - \", i_fold+1)\n",
        "  \n",
        "  for itr in range(iterations):\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in tqdm(trainloader, disable=True):\n",
        "      Y_pred = model(x_batch)\n",
        "      loss_batch = loss(Y_pred, y_batch)\n",
        "      op.zero_grad()\n",
        "      loss_batch.backward()\n",
        "      op.step()\n",
        "      avg_loss += loss_batch.item() / len(trainloader)\n",
        "\n",
        "    model.eval()\n",
        "    valid_pred_fold = np.zeros(X_val_fold.size(0))\n",
        "    test_pred_fold = np.zeros(len(X_test))\n",
        "    avg_val_loss = 0\n",
        "    for i, (x_batch, y_batch) in enumerate(validloader):\n",
        "      Y_pred = model(x_batch).detach()\n",
        "      avg_val_loss += loss(Y_pred, y_batch).item() / len(validloader)\n",
        "      valid_pred_fold[i * batch_size: (i+1) * batch_size] = (1 / (1+np.exp(-(Y_pred.cpu().numpy()))))[:, 0]\n",
        "\n",
        "    print(\"Iteration {} \\t loss={:.3f} \\t val_loss={:.3f} \".format(itr+1, avg_loss, avg_val_loss))\n",
        "\n",
        "  for i, (x_batch,) in enumerate(testloader):\n",
        "    Y_pred = model(x_batch).detach()\n",
        "    test_pred_fold[i * batch_size: (i+1) * batch_size] = (1 / (1+np.exp(-(Y_pred.cpu().numpy()))))[:, 0]\n",
        "\n",
        "  train_preds[valid_idx] = valid_pred_fold\n",
        "  test_preds += test_pred_fold / len(splits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold -  1\n",
            "Iteration 1 \t loss=86.310 \t val_loss=23.736 \n",
            "Iteration 2 \t loss=69.543 \t val_loss=20.769 \n",
            "Iteration 3 \t loss=58.701 \t val_loss=22.491 \n",
            "Iteration 4 \t loss=53.472 \t val_loss=24.759 \n",
            "Iteration 5 \t loss=49.549 \t val_loss=25.699 \n",
            "Iteration 6 \t loss=47.279 \t val_loss=25.122 \n",
            "Iteration 7 \t loss=45.423 \t val_loss=24.138 \n",
            "Iteration 8 \t loss=43.202 \t val_loss=23.015 \n",
            "Iteration 9 \t loss=33.254 \t val_loss=23.081 \n",
            "Iteration 10 \t loss=31.186 \t val_loss=22.584 \n",
            "Iteration 11 \t loss=38.093 \t val_loss=22.137 \n",
            "Iteration 12 \t loss=27.916 \t val_loss=21.183 \n",
            "Iteration 13 \t loss=29.781 \t val_loss=20.023 \n",
            "Iteration 14 \t loss=26.484 \t val_loss=20.357 \n",
            "Iteration 15 \t loss=26.005 \t val_loss=20.177 \n",
            "Iteration 16 \t loss=23.802 \t val_loss=19.767 \n",
            "Iteration 17 \t loss=22.578 \t val_loss=19.804 \n",
            "Iteration 18 \t loss=21.353 \t val_loss=19.399 \n",
            "Iteration 19 \t loss=22.604 \t val_loss=19.231 \n",
            "Iteration 20 \t loss=20.943 \t val_loss=19.097 \n",
            "Iteration 21 \t loss=22.397 \t val_loss=19.606 \n",
            "Iteration 22 \t loss=19.917 \t val_loss=19.369 \n",
            "Iteration 23 \t loss=21.238 \t val_loss=19.370 \n",
            "Iteration 24 \t loss=20.791 \t val_loss=19.094 \n",
            "Iteration 25 \t loss=19.914 \t val_loss=19.352 \n",
            "Iteration 26 \t loss=20.851 \t val_loss=19.134 \n",
            "Iteration 27 \t loss=19.585 \t val_loss=19.213 \n",
            "Iteration 28 \t loss=19.129 \t val_loss=19.198 \n",
            "Iteration 29 \t loss=19.607 \t val_loss=19.441 \n",
            "Iteration 30 \t loss=19.597 \t val_loss=19.179 \n",
            "Iteration 31 \t loss=19.127 \t val_loss=19.101 \n",
            "Iteration 32 \t loss=19.029 \t val_loss=19.184 \n",
            "Iteration 33 \t loss=18.723 \t val_loss=19.225 \n",
            "Iteration 34 \t loss=19.033 \t val_loss=19.274 \n",
            "Iteration 35 \t loss=18.746 \t val_loss=19.191 \n",
            "Iteration 36 \t loss=18.907 \t val_loss=19.196 \n",
            "Iteration 37 \t loss=19.260 \t val_loss=19.060 \n",
            "Iteration 38 \t loss=18.739 \t val_loss=19.077 \n",
            "Iteration 39 \t loss=19.087 \t val_loss=19.180 \n",
            "Iteration 40 \t loss=18.926 \t val_loss=19.100 \n",
            "Iteration 41 \t loss=19.017 \t val_loss=19.008 \n",
            "Iteration 42 \t loss=18.930 \t val_loss=19.045 \n",
            "Iteration 43 \t loss=18.548 \t val_loss=19.077 \n",
            "Iteration 44 \t loss=18.819 \t val_loss=18.987 \n",
            "Iteration 45 \t loss=18.938 \t val_loss=19.139 \n",
            "Iteration 46 \t loss=19.508 \t val_loss=19.051 \n",
            "Iteration 47 \t loss=18.540 \t val_loss=19.022 \n",
            "Iteration 48 \t loss=18.603 \t val_loss=19.065 \n",
            "Iteration 49 \t loss=18.592 \t val_loss=19.048 \n",
            "Iteration 50 \t loss=18.986 \t val_loss=19.104 \n",
            "Iteration 51 \t loss=18.796 \t val_loss=19.019 \n",
            "Iteration 52 \t loss=19.248 \t val_loss=18.952 \n",
            "Iteration 53 \t loss=19.056 \t val_loss=18.834 \n",
            "Iteration 54 \t loss=18.928 \t val_loss=18.901 \n",
            "Iteration 55 \t loss=18.734 \t val_loss=18.915 \n",
            "Iteration 56 \t loss=18.278 \t val_loss=19.139 \n",
            "Iteration 57 \t loss=18.849 \t val_loss=19.254 \n",
            "Iteration 58 \t loss=18.824 \t val_loss=18.856 \n",
            "Iteration 59 \t loss=19.085 \t val_loss=18.814 \n",
            "Iteration 60 \t loss=18.606 \t val_loss=18.974 \n",
            "Iteration 61 \t loss=18.906 \t val_loss=19.012 \n",
            "Iteration 62 \t loss=19.043 \t val_loss=18.964 \n",
            "Iteration 63 \t loss=18.853 \t val_loss=18.861 \n",
            "Iteration 64 \t loss=19.001 \t val_loss=18.823 \n",
            "Iteration 65 \t loss=18.617 \t val_loss=18.911 \n",
            "Iteration 66 \t loss=18.955 \t val_loss=18.742 \n",
            "Iteration 67 \t loss=18.814 \t val_loss=18.961 \n",
            "Iteration 68 \t loss=18.167 \t val_loss=19.021 \n",
            "Iteration 69 \t loss=18.594 \t val_loss=18.997 \n",
            "Iteration 70 \t loss=19.278 \t val_loss=18.957 \n",
            "Iteration 71 \t loss=19.127 \t val_loss=18.953 \n",
            "Iteration 72 \t loss=18.522 \t val_loss=18.974 \n",
            "Iteration 73 \t loss=18.621 \t val_loss=18.863 \n",
            "Iteration 74 \t loss=18.724 \t val_loss=18.958 \n",
            "Iteration 75 \t loss=18.702 \t val_loss=18.889 \n",
            "Iteration 76 \t loss=18.800 \t val_loss=18.966 \n",
            "Iteration 77 \t loss=18.536 \t val_loss=18.777 \n",
            "Iteration 78 \t loss=18.650 \t val_loss=18.764 \n",
            "Iteration 79 \t loss=18.395 \t val_loss=18.822 \n",
            "Iteration 80 \t loss=18.824 \t val_loss=19.089 \n",
            "Iteration 81 \t loss=18.516 \t val_loss=18.787 \n",
            "Iteration 82 \t loss=18.763 \t val_loss=18.757 \n",
            "Iteration 83 \t loss=18.684 \t val_loss=18.742 \n",
            "Iteration 84 \t loss=18.228 \t val_loss=18.818 \n",
            "Iteration 85 \t loss=18.334 \t val_loss=18.730 \n",
            "Iteration 86 \t loss=18.365 \t val_loss=18.765 \n",
            "Iteration 87 \t loss=18.347 \t val_loss=18.815 \n",
            "Iteration 88 \t loss=18.524 \t val_loss=18.885 \n",
            "Iteration 89 \t loss=18.409 \t val_loss=18.913 \n",
            "Iteration 90 \t loss=18.737 \t val_loss=18.749 \n",
            "Iteration 91 \t loss=18.242 \t val_loss=18.631 \n",
            "Iteration 92 \t loss=18.342 \t val_loss=18.746 \n",
            "Iteration 93 \t loss=18.540 \t val_loss=18.829 \n",
            "Iteration 94 \t loss=18.702 \t val_loss=18.878 \n",
            "Iteration 95 \t loss=19.028 \t val_loss=18.805 \n",
            "Iteration 96 \t loss=18.477 \t val_loss=18.672 \n",
            "Iteration 97 \t loss=18.216 \t val_loss=18.722 \n",
            "Iteration 98 \t loss=18.104 \t val_loss=18.678 \n",
            "Iteration 99 \t loss=18.359 \t val_loss=18.675 \n",
            "Iteration 100 \t loss=18.129 \t val_loss=18.619 \n",
            "Fold -  2\n",
            "Iteration 1 \t loss=19.308 \t val_loss=15.983 \n",
            "Iteration 2 \t loss=18.970 \t val_loss=15.483 \n",
            "Iteration 3 \t loss=19.463 \t val_loss=15.574 \n",
            "Iteration 4 \t loss=19.210 \t val_loss=15.560 \n",
            "Iteration 5 \t loss=19.264 \t val_loss=15.556 \n",
            "Iteration 6 \t loss=19.165 \t val_loss=15.486 \n",
            "Iteration 7 \t loss=19.107 \t val_loss=15.499 \n",
            "Iteration 8 \t loss=19.107 \t val_loss=15.727 \n",
            "Iteration 9 \t loss=19.093 \t val_loss=15.640 \n",
            "Iteration 10 \t loss=19.307 \t val_loss=15.490 \n",
            "Iteration 11 \t loss=19.059 \t val_loss=15.599 \n",
            "Iteration 12 \t loss=19.007 \t val_loss=15.384 \n",
            "Iteration 13 \t loss=18.765 \t val_loss=15.555 \n",
            "Iteration 14 \t loss=18.448 \t val_loss=15.676 \n",
            "Iteration 15 \t loss=18.823 \t val_loss=15.272 \n",
            "Iteration 16 \t loss=18.898 \t val_loss=15.292 \n",
            "Iteration 17 \t loss=18.830 \t val_loss=15.375 \n",
            "Iteration 18 \t loss=19.355 \t val_loss=15.184 \n",
            "Iteration 19 \t loss=18.817 \t val_loss=15.647 \n",
            "Iteration 20 \t loss=18.422 \t val_loss=15.180 \n",
            "Iteration 21 \t loss=18.573 \t val_loss=15.321 \n",
            "Iteration 22 \t loss=18.963 \t val_loss=15.316 \n",
            "Iteration 23 \t loss=18.872 \t val_loss=15.120 \n",
            "Iteration 24 \t loss=18.700 \t val_loss=15.342 \n",
            "Iteration 25 \t loss=18.476 \t val_loss=15.117 \n",
            "Iteration 26 \t loss=18.857 \t val_loss=15.177 \n",
            "Iteration 27 \t loss=18.448 \t val_loss=15.134 \n",
            "Iteration 28 \t loss=18.383 \t val_loss=15.352 \n",
            "Iteration 29 \t loss=18.253 \t val_loss=15.127 \n",
            "Iteration 30 \t loss=18.132 \t val_loss=15.091 \n",
            "Iteration 31 \t loss=18.703 \t val_loss=15.232 \n",
            "Iteration 32 \t loss=18.429 \t val_loss=14.988 \n",
            "Iteration 33 \t loss=18.253 \t val_loss=15.428 \n",
            "Iteration 34 \t loss=18.205 \t val_loss=15.115 \n",
            "Iteration 35 \t loss=18.420 \t val_loss=15.008 \n",
            "Iteration 36 \t loss=18.588 \t val_loss=15.010 \n",
            "Iteration 37 \t loss=18.291 \t val_loss=14.983 \n",
            "Iteration 38 \t loss=18.036 \t val_loss=15.066 \n",
            "Iteration 39 \t loss=18.965 \t val_loss=15.041 \n",
            "Iteration 40 \t loss=18.586 \t val_loss=14.976 \n",
            "Iteration 41 \t loss=18.150 \t val_loss=14.836 \n",
            "Iteration 42 \t loss=18.375 \t val_loss=14.791 \n",
            "Iteration 43 \t loss=17.537 \t val_loss=14.853 \n",
            "Iteration 44 \t loss=17.942 \t val_loss=14.824 \n",
            "Iteration 45 \t loss=17.815 \t val_loss=14.844 \n",
            "Iteration 46 \t loss=17.885 \t val_loss=14.902 \n",
            "Iteration 47 \t loss=17.943 \t val_loss=14.698 \n",
            "Iteration 48 \t loss=18.043 \t val_loss=14.631 \n",
            "Iteration 49 \t loss=18.031 \t val_loss=14.335 \n",
            "Iteration 50 \t loss=17.051 \t val_loss=14.119 \n",
            "Iteration 51 \t loss=18.299 \t val_loss=14.337 \n",
            "Iteration 52 \t loss=17.286 \t val_loss=14.351 \n",
            "Iteration 53 \t loss=17.766 \t val_loss=14.204 \n",
            "Iteration 54 \t loss=17.359 \t val_loss=14.216 \n",
            "Iteration 55 \t loss=17.283 \t val_loss=14.077 \n",
            "Iteration 56 \t loss=17.179 \t val_loss=14.092 \n",
            "Iteration 57 \t loss=17.751 \t val_loss=14.101 \n",
            "Iteration 58 \t loss=17.040 \t val_loss=14.123 \n",
            "Iteration 59 \t loss=16.889 \t val_loss=14.018 \n",
            "Iteration 60 \t loss=16.875 \t val_loss=14.067 \n",
            "Iteration 61 \t loss=17.124 \t val_loss=14.007 \n",
            "Iteration 62 \t loss=17.138 \t val_loss=13.945 \n",
            "Iteration 63 \t loss=17.401 \t val_loss=13.868 \n",
            "Iteration 64 \t loss=17.393 \t val_loss=13.798 \n",
            "Iteration 65 \t loss=16.539 \t val_loss=13.715 \n",
            "Iteration 66 \t loss=16.742 \t val_loss=13.780 \n",
            "Iteration 67 \t loss=17.087 \t val_loss=13.847 \n",
            "Iteration 68 \t loss=17.047 \t val_loss=13.963 \n",
            "Iteration 69 \t loss=17.375 \t val_loss=13.840 \n",
            "Iteration 70 \t loss=17.255 \t val_loss=13.727 \n",
            "Iteration 71 \t loss=16.730 \t val_loss=13.933 \n",
            "Iteration 72 \t loss=16.420 \t val_loss=13.649 \n",
            "Iteration 73 \t loss=17.011 \t val_loss=13.636 \n",
            "Iteration 74 \t loss=16.488 \t val_loss=13.506 \n",
            "Iteration 75 \t loss=16.908 \t val_loss=13.552 \n",
            "Iteration 76 \t loss=16.827 \t val_loss=13.531 \n",
            "Iteration 77 \t loss=16.378 \t val_loss=13.541 \n",
            "Iteration 78 \t loss=17.202 \t val_loss=13.696 \n",
            "Iteration 79 \t loss=16.633 \t val_loss=13.762 \n",
            "Iteration 80 \t loss=16.721 \t val_loss=13.381 \n",
            "Iteration 81 \t loss=16.637 \t val_loss=13.330 \n",
            "Iteration 82 \t loss=17.527 \t val_loss=13.788 \n",
            "Iteration 83 \t loss=16.506 \t val_loss=13.385 \n",
            "Iteration 84 \t loss=16.301 \t val_loss=13.355 \n",
            "Iteration 85 \t loss=16.565 \t val_loss=13.685 \n",
            "Iteration 86 \t loss=16.131 \t val_loss=13.295 \n",
            "Iteration 87 \t loss=17.077 \t val_loss=13.519 \n",
            "Iteration 88 \t loss=16.283 \t val_loss=13.446 \n",
            "Iteration 89 \t loss=16.245 \t val_loss=13.402 \n",
            "Iteration 90 \t loss=17.206 \t val_loss=13.574 \n",
            "Iteration 91 \t loss=16.593 \t val_loss=13.490 \n",
            "Iteration 92 \t loss=17.082 \t val_loss=13.541 \n",
            "Iteration 93 \t loss=15.874 \t val_loss=13.490 \n",
            "Iteration 94 \t loss=16.275 \t val_loss=13.359 \n",
            "Iteration 95 \t loss=16.355 \t val_loss=13.508 \n",
            "Iteration 96 \t loss=16.238 \t val_loss=13.240 \n",
            "Iteration 97 \t loss=15.951 \t val_loss=13.679 \n",
            "Iteration 98 \t loss=15.838 \t val_loss=13.203 \n",
            "Iteration 99 \t loss=16.467 \t val_loss=13.346 \n",
            "Iteration 100 \t loss=15.843 \t val_loss=13.435 \n",
            "Fold -  3\n",
            "Iteration 1 \t loss=17.278 \t val_loss=11.845 \n",
            "Iteration 2 \t loss=17.070 \t val_loss=12.047 \n",
            "Iteration 3 \t loss=16.968 \t val_loss=11.925 \n",
            "Iteration 4 \t loss=16.491 \t val_loss=11.944 \n",
            "Iteration 5 \t loss=16.100 \t val_loss=11.880 \n",
            "Iteration 6 \t loss=16.456 \t val_loss=11.691 \n",
            "Iteration 7 \t loss=16.549 \t val_loss=11.872 \n",
            "Iteration 8 \t loss=16.402 \t val_loss=11.835 \n",
            "Iteration 9 \t loss=16.290 \t val_loss=11.753 \n",
            "Iteration 10 \t loss=16.704 \t val_loss=11.952 \n",
            "Iteration 11 \t loss=16.609 \t val_loss=11.848 \n",
            "Iteration 12 \t loss=16.600 \t val_loss=12.038 \n",
            "Iteration 13 \t loss=16.566 \t val_loss=12.125 \n",
            "Iteration 14 \t loss=16.108 \t val_loss=11.969 \n",
            "Iteration 15 \t loss=15.958 \t val_loss=11.739 \n",
            "Iteration 16 \t loss=15.911 \t val_loss=11.579 \n",
            "Iteration 17 \t loss=16.486 \t val_loss=11.969 \n",
            "Iteration 18 \t loss=16.633 \t val_loss=11.762 \n",
            "Iteration 19 \t loss=16.312 \t val_loss=11.910 \n",
            "Iteration 20 \t loss=15.723 \t val_loss=11.773 \n",
            "Iteration 21 \t loss=16.419 \t val_loss=11.813 \n",
            "Iteration 22 \t loss=16.670 \t val_loss=11.822 \n",
            "Iteration 23 \t loss=16.040 \t val_loss=11.735 \n",
            "Iteration 24 \t loss=16.558 \t val_loss=11.669 \n",
            "Iteration 25 \t loss=15.973 \t val_loss=11.777 \n",
            "Iteration 26 \t loss=15.813 \t val_loss=11.798 \n",
            "Iteration 27 \t loss=15.887 \t val_loss=11.648 \n",
            "Iteration 28 \t loss=16.427 \t val_loss=11.668 \n",
            "Iteration 29 \t loss=15.888 \t val_loss=11.718 \n",
            "Iteration 30 \t loss=15.981 \t val_loss=11.772 \n",
            "Iteration 31 \t loss=16.439 \t val_loss=11.894 \n",
            "Iteration 32 \t loss=16.059 \t val_loss=11.926 \n",
            "Iteration 33 \t loss=15.830 \t val_loss=11.923 \n",
            "Iteration 34 \t loss=15.964 \t val_loss=11.593 \n",
            "Iteration 35 \t loss=15.705 \t val_loss=11.656 \n",
            "Iteration 36 \t loss=15.631 \t val_loss=11.820 \n",
            "Iteration 37 \t loss=15.666 \t val_loss=11.885 \n",
            "Iteration 38 \t loss=15.696 \t val_loss=11.601 \n",
            "Iteration 39 \t loss=15.629 \t val_loss=11.458 \n",
            "Iteration 40 \t loss=15.963 \t val_loss=11.710 \n",
            "Iteration 41 \t loss=16.395 \t val_loss=11.874 \n",
            "Iteration 42 \t loss=15.712 \t val_loss=11.746 \n",
            "Iteration 43 \t loss=15.686 \t val_loss=11.679 \n",
            "Iteration 44 \t loss=15.503 \t val_loss=11.615 \n",
            "Iteration 45 \t loss=15.850 \t val_loss=11.517 \n",
            "Iteration 46 \t loss=15.388 \t val_loss=11.448 \n",
            "Iteration 47 \t loss=15.555 \t val_loss=11.797 \n",
            "Iteration 48 \t loss=16.031 \t val_loss=11.747 \n",
            "Iteration 49 \t loss=15.404 \t val_loss=11.705 \n",
            "Iteration 50 \t loss=15.699 \t val_loss=11.534 \n",
            "Iteration 51 \t loss=15.290 \t val_loss=11.549 \n",
            "Iteration 52 \t loss=16.162 \t val_loss=11.718 \n",
            "Iteration 53 \t loss=15.548 \t val_loss=11.713 \n",
            "Iteration 54 \t loss=15.302 \t val_loss=11.497 \n",
            "Iteration 55 \t loss=15.757 \t val_loss=11.481 \n",
            "Iteration 56 \t loss=15.624 \t val_loss=11.520 \n",
            "Iteration 57 \t loss=15.005 \t val_loss=11.425 \n",
            "Iteration 58 \t loss=15.233 \t val_loss=11.460 \n",
            "Iteration 59 \t loss=15.736 \t val_loss=11.758 \n",
            "Iteration 60 \t loss=15.768 \t val_loss=11.486 \n",
            "Iteration 61 \t loss=15.853 \t val_loss=11.671 \n",
            "Iteration 62 \t loss=15.346 \t val_loss=11.666 \n",
            "Iteration 63 \t loss=15.949 \t val_loss=11.548 \n",
            "Iteration 64 \t loss=15.318 \t val_loss=11.441 \n",
            "Iteration 65 \t loss=15.329 \t val_loss=11.357 \n",
            "Iteration 66 \t loss=15.497 \t val_loss=11.533 \n",
            "Iteration 67 \t loss=15.144 \t val_loss=11.539 \n",
            "Iteration 68 \t loss=15.659 \t val_loss=11.600 \n",
            "Iteration 69 \t loss=15.542 \t val_loss=11.538 \n",
            "Iteration 70 \t loss=15.175 \t val_loss=11.576 \n",
            "Iteration 71 \t loss=15.473 \t val_loss=11.499 \n",
            "Iteration 72 \t loss=15.037 \t val_loss=11.506 \n",
            "Iteration 73 \t loss=15.026 \t val_loss=11.422 \n",
            "Iteration 74 \t loss=15.375 \t val_loss=11.233 \n",
            "Iteration 75 \t loss=15.616 \t val_loss=11.578 \n",
            "Iteration 76 \t loss=15.775 \t val_loss=11.638 \n",
            "Iteration 77 \t loss=15.339 \t val_loss=11.458 \n",
            "Iteration 78 \t loss=15.873 \t val_loss=11.576 \n",
            "Iteration 79 \t loss=15.928 \t val_loss=11.619 \n",
            "Iteration 80 \t loss=15.331 \t val_loss=11.543 \n",
            "Iteration 81 \t loss=15.487 \t val_loss=11.767 \n",
            "Iteration 82 \t loss=14.842 \t val_loss=11.570 \n",
            "Iteration 83 \t loss=15.260 \t val_loss=11.532 \n",
            "Iteration 84 \t loss=15.029 \t val_loss=11.523 \n",
            "Iteration 85 \t loss=14.926 \t val_loss=11.548 \n",
            "Iteration 86 \t loss=15.184 \t val_loss=11.283 \n",
            "Iteration 87 \t loss=15.125 \t val_loss=11.426 \n",
            "Iteration 88 \t loss=15.211 \t val_loss=11.408 \n",
            "Iteration 89 \t loss=15.227 \t val_loss=11.347 \n",
            "Iteration 90 \t loss=15.246 \t val_loss=11.614 \n",
            "Iteration 91 \t loss=15.115 \t val_loss=11.656 \n",
            "Iteration 92 \t loss=15.489 \t val_loss=11.466 \n",
            "Iteration 93 \t loss=15.210 \t val_loss=11.647 \n",
            "Iteration 94 \t loss=15.643 \t val_loss=11.666 \n",
            "Iteration 95 \t loss=15.278 \t val_loss=11.598 \n",
            "Iteration 96 \t loss=15.255 \t val_loss=11.564 \n",
            "Iteration 97 \t loss=15.811 \t val_loss=11.502 \n",
            "Iteration 98 \t loss=15.379 \t val_loss=11.454 \n",
            "Iteration 99 \t loss=15.305 \t val_loss=11.402 \n",
            "Iteration 100 \t loss=15.102 \t val_loss=11.389 \n",
            "Fold -  4\n",
            "Iteration 1 \t loss=14.035 \t val_loss=13.645 \n",
            "Iteration 2 \t loss=14.440 \t val_loss=13.798 \n",
            "Iteration 3 \t loss=14.438 \t val_loss=13.454 \n",
            "Iteration 4 \t loss=15.464 \t val_loss=13.580 \n",
            "Iteration 5 \t loss=14.381 \t val_loss=14.137 \n",
            "Iteration 6 \t loss=14.632 \t val_loss=13.605 \n",
            "Iteration 7 \t loss=14.281 \t val_loss=13.925 \n",
            "Iteration 8 \t loss=14.389 \t val_loss=14.024 \n",
            "Iteration 9 \t loss=14.794 \t val_loss=13.580 \n",
            "Iteration 10 \t loss=14.554 \t val_loss=13.977 \n",
            "Iteration 11 \t loss=14.598 \t val_loss=13.945 \n",
            "Iteration 12 \t loss=14.038 \t val_loss=13.692 \n",
            "Iteration 13 \t loss=14.610 \t val_loss=13.801 \n",
            "Iteration 14 \t loss=14.602 \t val_loss=13.875 \n",
            "Iteration 15 \t loss=14.440 \t val_loss=13.834 \n",
            "Iteration 16 \t loss=14.611 \t val_loss=13.854 \n",
            "Iteration 17 \t loss=14.627 \t val_loss=13.702 \n",
            "Iteration 18 \t loss=14.618 \t val_loss=13.881 \n",
            "Iteration 19 \t loss=14.622 \t val_loss=13.832 \n",
            "Iteration 20 \t loss=14.227 \t val_loss=13.813 \n",
            "Iteration 21 \t loss=14.760 \t val_loss=14.102 \n",
            "Iteration 22 \t loss=14.033 \t val_loss=14.117 \n",
            "Iteration 23 \t loss=14.326 \t val_loss=14.022 \n",
            "Iteration 24 \t loss=13.858 \t val_loss=14.270 \n",
            "Iteration 25 \t loss=14.997 \t val_loss=13.946 \n",
            "Iteration 26 \t loss=14.613 \t val_loss=14.059 \n",
            "Iteration 27 \t loss=14.072 \t val_loss=14.075 \n",
            "Iteration 28 \t loss=14.304 \t val_loss=14.123 \n",
            "Iteration 29 \t loss=14.239 \t val_loss=14.103 \n",
            "Iteration 30 \t loss=14.392 \t val_loss=14.474 \n",
            "Iteration 31 \t loss=14.540 \t val_loss=14.318 \n",
            "Iteration 32 \t loss=14.594 \t val_loss=14.098 \n",
            "Iteration 33 \t loss=13.644 \t val_loss=14.084 \n",
            "Iteration 34 \t loss=14.247 \t val_loss=14.249 \n",
            "Iteration 35 \t loss=14.289 \t val_loss=14.006 \n",
            "Iteration 36 \t loss=13.809 \t val_loss=14.311 \n",
            "Iteration 37 \t loss=14.461 \t val_loss=14.041 \n",
            "Iteration 38 \t loss=13.984 \t val_loss=13.999 \n",
            "Iteration 39 \t loss=13.973 \t val_loss=13.948 \n",
            "Iteration 40 \t loss=14.626 \t val_loss=13.989 \n",
            "Iteration 41 \t loss=14.587 \t val_loss=13.916 \n",
            "Iteration 42 \t loss=14.651 \t val_loss=13.892 \n",
            "Iteration 43 \t loss=14.543 \t val_loss=14.171 \n",
            "Iteration 44 \t loss=14.226 \t val_loss=13.850 \n",
            "Iteration 45 \t loss=14.571 \t val_loss=13.953 \n",
            "Iteration 46 \t loss=14.184 \t val_loss=13.743 \n",
            "Iteration 47 \t loss=13.998 \t val_loss=13.901 \n",
            "Iteration 48 \t loss=14.599 \t val_loss=14.188 \n",
            "Iteration 49 \t loss=14.940 \t val_loss=13.935 \n",
            "Iteration 50 \t loss=14.745 \t val_loss=13.748 \n",
            "Iteration 51 \t loss=14.221 \t val_loss=14.024 \n",
            "Iteration 52 \t loss=15.213 \t val_loss=13.812 \n",
            "Iteration 53 \t loss=14.034 \t val_loss=13.801 \n",
            "Iteration 54 \t loss=13.824 \t val_loss=14.019 \n",
            "Iteration 55 \t loss=13.876 \t val_loss=13.850 \n",
            "Iteration 56 \t loss=14.196 \t val_loss=13.752 \n",
            "Iteration 57 \t loss=14.022 \t val_loss=13.977 \n",
            "Iteration 58 \t loss=14.975 \t val_loss=14.235 \n",
            "Iteration 59 \t loss=14.095 \t val_loss=13.757 \n",
            "Iteration 60 \t loss=14.434 \t val_loss=13.928 \n",
            "Iteration 61 \t loss=14.166 \t val_loss=13.923 \n",
            "Iteration 62 \t loss=14.613 \t val_loss=13.865 \n",
            "Iteration 63 \t loss=13.903 \t val_loss=13.985 \n",
            "Iteration 64 \t loss=14.580 \t val_loss=13.910 \n",
            "Iteration 65 \t loss=14.072 \t val_loss=13.958 \n",
            "Iteration 66 \t loss=14.501 \t val_loss=14.123 \n",
            "Iteration 67 \t loss=14.460 \t val_loss=14.258 \n",
            "Iteration 68 \t loss=14.294 \t val_loss=13.821 \n",
            "Iteration 69 \t loss=13.987 \t val_loss=13.790 \n",
            "Iteration 70 \t loss=14.659 \t val_loss=13.812 \n",
            "Iteration 71 \t loss=14.398 \t val_loss=13.879 \n",
            "Iteration 72 \t loss=14.037 \t val_loss=13.896 \n",
            "Iteration 73 \t loss=14.288 \t val_loss=13.987 \n",
            "Iteration 74 \t loss=14.102 \t val_loss=13.925 \n",
            "Iteration 75 \t loss=14.111 \t val_loss=13.829 \n",
            "Iteration 76 \t loss=13.966 \t val_loss=13.923 \n",
            "Iteration 77 \t loss=14.414 \t val_loss=13.968 \n",
            "Iteration 78 \t loss=14.669 \t val_loss=14.114 \n",
            "Iteration 79 \t loss=14.360 \t val_loss=14.284 \n",
            "Iteration 80 \t loss=13.662 \t val_loss=14.114 \n",
            "Iteration 81 \t loss=14.086 \t val_loss=14.133 \n",
            "Iteration 82 \t loss=13.971 \t val_loss=13.918 \n",
            "Iteration 83 \t loss=14.335 \t val_loss=13.853 \n",
            "Iteration 84 \t loss=14.221 \t val_loss=13.913 \n",
            "Iteration 85 \t loss=14.289 \t val_loss=13.760 \n",
            "Iteration 86 \t loss=14.269 \t val_loss=13.592 \n",
            "Iteration 87 \t loss=14.076 \t val_loss=13.655 \n",
            "Iteration 88 \t loss=14.410 \t val_loss=13.978 \n",
            "Iteration 89 \t loss=14.418 \t val_loss=14.075 \n",
            "Iteration 90 \t loss=14.100 \t val_loss=14.039 \n",
            "Iteration 91 \t loss=14.199 \t val_loss=13.981 \n",
            "Iteration 92 \t loss=14.197 \t val_loss=14.043 \n",
            "Iteration 93 \t loss=14.010 \t val_loss=14.017 \n",
            "Iteration 94 \t loss=14.268 \t val_loss=13.913 \n",
            "Iteration 95 \t loss=14.177 \t val_loss=13.838 \n",
            "Iteration 96 \t loss=14.499 \t val_loss=13.829 \n",
            "Iteration 97 \t loss=14.306 \t val_loss=13.736 \n",
            "Iteration 98 \t loss=14.604 \t val_loss=13.764 \n",
            "Iteration 99 \t loss=14.548 \t val_loss=13.871 \n",
            "Iteration 100 \t loss=14.357 \t val_loss=13.681 \n",
            "Fold -  5\n",
            "Iteration 1 \t loss=15.087 \t val_loss=11.857 \n",
            "Iteration 2 \t loss=14.791 \t val_loss=12.021 \n",
            "Iteration 3 \t loss=14.545 \t val_loss=12.386 \n",
            "Iteration 4 \t loss=14.617 \t val_loss=11.907 \n",
            "Iteration 5 \t loss=14.583 \t val_loss=12.117 \n",
            "Iteration 6 \t loss=14.532 \t val_loss=11.974 \n",
            "Iteration 7 \t loss=14.258 \t val_loss=12.037 \n",
            "Iteration 8 \t loss=14.570 \t val_loss=12.012 \n",
            "Iteration 9 \t loss=14.643 \t val_loss=11.783 \n",
            "Iteration 10 \t loss=14.181 \t val_loss=12.097 \n",
            "Iteration 11 \t loss=14.398 \t val_loss=11.940 \n",
            "Iteration 12 \t loss=14.617 \t val_loss=11.825 \n",
            "Iteration 13 \t loss=14.800 \t val_loss=11.847 \n",
            "Iteration 14 \t loss=14.512 \t val_loss=12.462 \n",
            "Iteration 15 \t loss=14.428 \t val_loss=11.944 \n",
            "Iteration 16 \t loss=14.904 \t val_loss=11.958 \n",
            "Iteration 17 \t loss=14.698 \t val_loss=11.870 \n",
            "Iteration 18 \t loss=14.914 \t val_loss=12.103 \n",
            "Iteration 19 \t loss=14.817 \t val_loss=11.937 \n",
            "Iteration 20 \t loss=14.566 \t val_loss=12.052 \n",
            "Iteration 21 \t loss=14.585 \t val_loss=11.979 \n",
            "Iteration 22 \t loss=14.540 \t val_loss=12.139 \n",
            "Iteration 23 \t loss=14.900 \t val_loss=12.327 \n",
            "Iteration 24 \t loss=14.274 \t val_loss=11.855 \n",
            "Iteration 25 \t loss=14.542 \t val_loss=12.207 \n",
            "Iteration 26 \t loss=14.785 \t val_loss=12.028 \n",
            "Iteration 27 \t loss=14.476 \t val_loss=11.956 \n",
            "Iteration 28 \t loss=14.631 \t val_loss=12.201 \n",
            "Iteration 29 \t loss=14.596 \t val_loss=12.284 \n",
            "Iteration 30 \t loss=14.947 \t val_loss=11.944 \n",
            "Iteration 31 \t loss=14.880 \t val_loss=12.063 \n",
            "Iteration 32 \t loss=14.677 \t val_loss=12.043 \n",
            "Iteration 33 \t loss=14.317 \t val_loss=11.982 \n",
            "Iteration 34 \t loss=14.600 \t val_loss=12.223 \n",
            "Iteration 35 \t loss=14.618 \t val_loss=12.064 \n",
            "Iteration 36 \t loss=14.530 \t val_loss=12.058 \n",
            "Iteration 37 \t loss=14.324 \t val_loss=11.930 \n",
            "Iteration 38 \t loss=14.743 \t val_loss=12.248 \n",
            "Iteration 39 \t loss=14.313 \t val_loss=12.412 \n",
            "Iteration 40 \t loss=14.782 \t val_loss=12.368 \n",
            "Iteration 41 \t loss=14.532 \t val_loss=11.950 \n",
            "Iteration 42 \t loss=14.256 \t val_loss=12.209 \n",
            "Iteration 43 \t loss=14.382 \t val_loss=12.008 \n",
            "Iteration 44 \t loss=14.305 \t val_loss=12.219 \n",
            "Iteration 45 \t loss=14.442 \t val_loss=12.139 \n",
            "Iteration 46 \t loss=14.344 \t val_loss=12.060 \n",
            "Iteration 47 \t loss=14.418 \t val_loss=12.291 \n",
            "Iteration 48 \t loss=14.502 \t val_loss=12.169 \n",
            "Iteration 49 \t loss=14.209 \t val_loss=12.055 \n",
            "Iteration 50 \t loss=14.824 \t val_loss=12.243 \n",
            "Iteration 51 \t loss=14.211 \t val_loss=12.096 \n",
            "Iteration 52 \t loss=14.326 \t val_loss=12.018 \n",
            "Iteration 53 \t loss=14.526 \t val_loss=12.260 \n",
            "Iteration 54 \t loss=14.193 \t val_loss=12.089 \n",
            "Iteration 55 \t loss=14.769 \t val_loss=12.079 \n",
            "Iteration 56 \t loss=15.177 \t val_loss=12.080 \n",
            "Iteration 57 \t loss=14.541 \t val_loss=12.047 \n",
            "Iteration 58 \t loss=14.791 \t val_loss=12.591 \n",
            "Iteration 59 \t loss=14.491 \t val_loss=12.111 \n",
            "Iteration 60 \t loss=15.088 \t val_loss=12.049 \n",
            "Iteration 61 \t loss=14.570 \t val_loss=12.259 \n",
            "Iteration 62 \t loss=14.422 \t val_loss=12.167 \n",
            "Iteration 63 \t loss=14.476 \t val_loss=12.406 \n",
            "Iteration 64 \t loss=14.679 \t val_loss=12.306 \n",
            "Iteration 65 \t loss=14.635 \t val_loss=12.031 \n",
            "Iteration 66 \t loss=14.221 \t val_loss=12.224 \n",
            "Iteration 67 \t loss=14.240 \t val_loss=12.120 \n",
            "Iteration 68 \t loss=14.437 \t val_loss=11.947 \n",
            "Iteration 69 \t loss=14.344 \t val_loss=12.303 \n",
            "Iteration 70 \t loss=14.608 \t val_loss=12.011 \n",
            "Iteration 71 \t loss=14.269 \t val_loss=12.174 \n",
            "Iteration 72 \t loss=15.186 \t val_loss=12.294 \n",
            "Iteration 73 \t loss=14.767 \t val_loss=12.497 \n",
            "Iteration 74 \t loss=14.115 \t val_loss=11.994 \n",
            "Iteration 75 \t loss=14.150 \t val_loss=12.391 \n",
            "Iteration 76 \t loss=14.262 \t val_loss=12.139 \n",
            "Iteration 77 \t loss=14.310 \t val_loss=12.093 \n",
            "Iteration 78 \t loss=14.215 \t val_loss=12.176 \n",
            "Iteration 79 \t loss=13.711 \t val_loss=12.280 \n",
            "Iteration 80 \t loss=14.692 \t val_loss=12.058 \n",
            "Iteration 81 \t loss=14.023 \t val_loss=12.051 \n",
            "Iteration 82 \t loss=14.330 \t val_loss=11.984 \n",
            "Iteration 83 \t loss=14.850 \t val_loss=12.052 \n",
            "Iteration 84 \t loss=15.211 \t val_loss=12.170 \n",
            "Iteration 85 \t loss=14.524 \t val_loss=12.022 \n",
            "Iteration 86 \t loss=14.276 \t val_loss=11.990 \n",
            "Iteration 87 \t loss=14.732 \t val_loss=12.254 \n",
            "Iteration 88 \t loss=14.535 \t val_loss=12.100 \n",
            "Iteration 89 \t loss=14.119 \t val_loss=12.487 \n",
            "Iteration 90 \t loss=14.249 \t val_loss=12.629 \n",
            "Iteration 91 \t loss=14.699 \t val_loss=12.105 \n",
            "Iteration 92 \t loss=13.919 \t val_loss=11.954 \n",
            "Iteration 93 \t loss=14.693 \t val_loss=11.859 \n",
            "Iteration 94 \t loss=14.117 \t val_loss=12.241 \n",
            "Iteration 95 \t loss=14.898 \t val_loss=12.231 \n",
            "Iteration 96 \t loss=14.610 \t val_loss=12.185 \n",
            "Iteration 97 \t loss=14.133 \t val_loss=12.105 \n",
            "Iteration 98 \t loss=14.238 \t val_loss=12.325 \n",
            "Iteration 99 \t loss=14.427 \t val_loss=12.035 \n",
            "Iteration 100 \t loss=14.053 \t val_loss=12.126 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiLjt3hKjSsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc02178-0487-4431-e316-c319839eea66"
      },
      "source": [
        "best_threshold = 0\n",
        "best_score = 0\n",
        "\n",
        "for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
        "  score = accuracy_score(y_true=Y_train, y_pred=train_preds > threshold)\n",
        "  if score > best_score:\n",
        "    best_threshold = threshold\n",
        "    best_score = score\n",
        "\n",
        "#search_result = {'Threshold': best_threshold, 'Accuracy': best_score}\n",
        "print(\"Accuracy: {} \\t Threshold: {}\".format(best_score, best_threshold))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 2832.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7710674157303371 \t Threshold: 0.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC-SQfkyonA7",
        "outputId": "33bea023-ed11-4f42-b1d8-9f92e81d7dbc"
      },
      "source": [
        "epoch_acc = {}\n",
        "\n",
        "for count_itrs in range(100, 1100, 100):\n",
        "  print(\"Running for {} epochs\".format(count_itrs))\n",
        "\n",
        "  for i_fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "    X_train_fold = torch.tensor(X_train[train_idx], dtype=torch.float32).cuda()\n",
        "    Y_train_fold = torch.tensor(Y_train[train_idx], dtype=torch.float32).unsqueeze(1).cuda()\n",
        "    X_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.float32).cuda()\n",
        "    Y_val_fold = torch.tensor(Y_train[valid_idx], dtype=torch.float32).unsqueeze(1).cuda()\n",
        "\n",
        "    train = torch.utils.data.TensorDataset(X_train_fold, Y_train_fold)\n",
        "    valid = torch.utils.data.TensorDataset(X_val_fold, Y_val_fold)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    validloader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = NN_class\n",
        "    NN_class.cuda()\n",
        "    loss = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
        "    op = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    print(\"\\t Fold - \", i_fold+1)\n",
        "    \n",
        "    for itr in range(count_itrs):\n",
        "      model.train()\n",
        "      avg_loss = 0\n",
        "\n",
        "      for x_batch, y_batch in tqdm(trainloader, disable=True):\n",
        "        Y_pred = model(x_batch)\n",
        "        loss_batch = loss(Y_pred, y_batch)\n",
        "        op.zero_grad()\n",
        "        loss_batch.backward()\n",
        "        op.step()\n",
        "        avg_loss += loss_batch.item() / len(trainloader)\n",
        "\n",
        "      model.eval()\n",
        "      valid_pred_fold = np.zeros(X_val_fold.size(0))\n",
        "      test_pred_fold = np.zeros(len(X_test))\n",
        "      avg_val_loss = 0\n",
        "      for i, (x_batch, y_batch) in enumerate(validloader):\n",
        "        Y_pred = model(x_batch).detach()\n",
        "        avg_val_loss += loss(Y_pred, y_batch).item() / len(validloader)\n",
        "        valid_pred_fold[i * batch_size: (i+1) * batch_size] = (1 / (1+np.exp(-(Y_pred.cpu().numpy()))))[:, 0]\n",
        "\n",
        "    for i, (x_batch,) in enumerate(testloader):\n",
        "      Y_pred = model(x_batch).detach()\n",
        "      test_pred_fold[i * batch_size: (i+1) * batch_size] = (1 / (1+np.exp(-(Y_pred.cpu().numpy()))))[:, 0]\n",
        "\n",
        "    train_preds[valid_idx] = valid_pred_fold\n",
        "    test_preds += test_pred_fold / len(splits)\n",
        "\n",
        "  best_threshold = 0\n",
        "  best_score = 0\n",
        "\n",
        "  for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
        "    score = accuracy_score(y_true=Y_train, y_pred=train_preds > threshold)\n",
        "    if score > best_score:\n",
        "      best_threshold = threshold\n",
        "      best_score = score\n",
        "\n",
        "  epoch_acc[count_itrs] = best_score\n",
        "  #search_result = {'Threshold': best_threshold, 'Accuracy': best_score}\n",
        "  print(\"Accuracy: {} \\t Threshold: {}\".format(best_score, best_threshold))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for 100 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n",
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3516.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8426966292134831 \t Threshold: 0.43\n",
            "Running for 200 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n",
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3385.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8258426966292135 \t Threshold: 0.41000000000000003\n",
            "Running for 300 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n",
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3474.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8314606741573034 \t Threshold: 0.44\n",
            "Running for 400 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3154.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8342696629213483 \t Threshold: 0.45\n",
            "Running for 500 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n",
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3398.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.827247191011236 \t Threshold: 0.43\n",
            "Running for 600 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3080.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8258426966292135 \t Threshold: 0.45\n",
            "Running for 700 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 2917.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8230337078651685 \t Threshold: 0.45\n",
            "Running for 800 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3629.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.827247191011236 \t Threshold: 0.43\n",
            "Running for 900 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n",
            "\t Fold -  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3473.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8258426966292135 \t Threshold: 0.47000000000000003\n",
            "Running for 1000 epochs\n",
            "\t Fold -  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Fold -  3\n",
            "\t Fold -  4\n",
            "\t Fold -  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3651.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824438202247191 \t Threshold: 0.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "LTak3Aodr25s",
        "outputId": "a6366adf-a372-4289-e0e2-51a256bbba06"
      },
      "source": [
        "plot_x = []\n",
        "plot_y = []\n",
        "for key in epoch_acc.keys():\n",
        "  plot_x.append(key)\n",
        "\n",
        "for value in epoch_acc.values():\n",
        "  plot_y.append(value)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(plot_x, plot_y)\n",
        "plt.title('Accuracy for different epochs')\n",
        "plt.xlabel('Number of epochs')\n",
        "plt.ylabel('Accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}